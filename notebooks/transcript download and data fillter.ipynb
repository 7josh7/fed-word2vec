{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094d627c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading FOMC documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing calendar pages: 100%|██████████| 126/126 [00:15<00:00,  8.02it/s]\n",
      "Downloading FOMC docs: 100%|██████████| 164/164 [00:00<00:00, 321.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading speeches...\n",
      "Attempting yearly archives...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading speeches: 100%|██████████| 249/249 [00:00<00:00, 7917.26it/s]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary: FOMC docs: 164; speeches: 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FOMC & Fed Speech Downloader\n",
    "============================\n",
    "Downloads FOMC statements/minutes, press conferences, and Fed Chair speeches.\n",
    "Core logic lives in src/downloader.py; this notebook orchestrates the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "from src.downloader import create_session, collect_fomc_documents, collect_speeches\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(\"../data/fed_downloads\")\n",
    "\n",
    "# Create session and download\n",
    "session = create_session()\n",
    "\n",
    "# Download FOMC documents\n",
    "print(\"Downloading FOMC documents...\")\n",
    "fomc_records = collect_fomc_documents(session, DATA_DIR / \"fomc\")\n",
    "\n",
    "# Download speeches\n",
    "print(\"\\nDownloading speeches...\")\n",
    "speech_records = collect_speeches(session, DATA_DIR / \"speeches\", start_year=1994)\n",
    "\n",
    "print(f\"\\nSummary: FOMC docs: {len(fomc_records)}; speeches: {len(speech_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab15ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Processing FOMC documents (line-level whitelist-first + dynamic guard)...\n",
      "============================================================\n",
      "Removed lines saved to: ..\\data\\fed_text\\removed_lines.txt\n",
      "Corpus removal profile (FOMC):\n",
      "  Mean: 2.0% | Median: 0.0% | 90th: 5.9%\n",
      "  Recommended guard: 30% (clamped 30–60%)\n",
      "\n",
      "Processed files: 109\n",
      "Average length (chars): 24166.3\n",
      "Average tokens: 3691.6\n",
      "Top tokens:\n",
      "  the: 27964\n",
      "  of: 14591\n",
      "  and: 11445\n",
      "  to: 11129\n",
      "  in: 11097\n",
      "  that: 6626\n",
      "  a: 4871\n",
      "  for: 3913\n",
      "  on: 3791\n",
      "  participants: 3495\n",
      "Output saved to: ..\\data\\fed_text\\fomc\n",
      "\n",
      "============================================================\n",
      "Processing speeches...\n",
      "============================================================\n",
      "Processed files: 242\n",
      "Average length (chars): 17933.5\n",
      "Average tokens: 2835.8\n",
      "Top tokens:\n",
      "  the: 44322\n",
      "  of: 23569\n",
      "  and: 21959\n",
      "  to: 18750\n",
      "  in: 15903\n",
      "  a: 10338\n",
      "  that: 9002\n",
      "  for: 7457\n",
      "  is: 5880\n",
      "  as: 5120\n",
      "Output saved to: ..\\data\\fed_text\\speeches\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text Extraction & Cleaning\n",
    "==========================\n",
    "Parses raw HTML into cleaned text for FOMC minutes and speeches.\n",
    "Core logic lives in src/text_cleaner.py; this notebook orchestrates the pipeline.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from src.text_cleaner import process_tree, recommend_guard_from_profile\n",
    "\n",
    "# Paths\n",
    "INPUT_FOMC = Path(\"../data/fed_downloads/fomc\")\n",
    "OUTPUT_FOMC = Path(\"../data/fed_text/fomc\")\n",
    "INPUT_SPEECHES = Path(\"../data/fed_downloads/speeches\")\n",
    "OUTPUT_SPEECHES = Path(\"../data/fed_text/speeches\")\n",
    "REMOVED_LINES_FILE = Path(\"../data/fed_text/removed_lines.txt\")\n",
    "\n",
    "# Process FOMC documents\n",
    "print(\"=\" * 60)\n",
    "print(\"Processing FOMC documents (line-level whitelist-first + dynamic guard)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if INPUT_FOMC.exists():\n",
    "    # Profile pass to compute corpus ratios and collect removed lines\n",
    "    removed_buffer: list[str] = []\n",
    "    total_files, total_chars, total_tokens, counter, ratios = process_tree(\n",
    "        INPUT_FOMC, OUTPUT_FOMC, is_fomc=True, removal_guard=None, removed_collector=removed_buffer\n",
    "    )\n",
    "    \n",
    "    # Save removed lines for inspection\n",
    "    if removed_buffer:\n",
    "        REMOVED_LINES_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "        REMOVED_LINES_FILE.write_text(\"\\n\".join(removed_buffer), encoding=\"utf-8\")\n",
    "        print(f\"Removed lines saved to: {REMOVED_LINES_FILE}\")\n",
    "    \n",
    "    # Compute recommended guard and re-run if needed\n",
    "    if ratios:\n",
    "        recommended_guard = recommend_guard_from_profile(ratios)\n",
    "        total_files, total_chars, total_tokens, counter, _ = process_tree(\n",
    "            INPUT_FOMC, OUTPUT_FOMC, is_fomc=True, removal_guard=recommended_guard\n",
    "        )\n",
    "    \n",
    "    # Report stats\n",
    "    print(f\"\\nProcessed files: {total_files}\")\n",
    "    if total_files:\n",
    "        print(f\"Average length (chars): {total_chars / total_files:.1f}\")\n",
    "        print(f\"Average tokens: {total_tokens / total_files:.1f}\")\n",
    "        print(\"Top tokens:\")\n",
    "        for tok, cnt in counter.most_common(10):\n",
    "            print(f\"  {tok}: {cnt}\")\n",
    "    print(f\"Output saved to: {OUTPUT_FOMC}\")\n",
    "else:\n",
    "    print(f\"Input directory not found: {INPUT_FOMC}\")\n",
    "\n",
    "# Process speeches\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Processing speeches...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if INPUT_SPEECHES.exists():\n",
    "    total_files, total_chars, total_tokens, counter, _ = process_tree(\n",
    "        INPUT_SPEECHES, OUTPUT_SPEECHES, is_fomc=False, removal_guard=None\n",
    "    )\n",
    "    print(f\"Processed files: {total_files}\")\n",
    "    if total_files:\n",
    "        print(f\"Average length (chars): {total_chars / total_files:.1f}\")\n",
    "        print(f\"Average tokens: {total_tokens / total_files:.1f}\")\n",
    "        print(\"Top tokens:\")\n",
    "        for tok, cnt in counter.most_common(10):\n",
    "            print(f\"  {tok}: {cnt}\")\n",
    "    print(f\"Output saved to: {OUTPUT_SPEECHES}\")\n",
    "else:\n",
    "    print(f\"Input directory not found: {INPUT_SPEECHES}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
